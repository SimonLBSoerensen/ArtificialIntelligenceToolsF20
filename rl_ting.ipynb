{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import bottleneck as bn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTabel:\n",
    "    def __init__(self, learning_rate, discount_factor, epsilon, start_eps_decay, end_eps_decay, eps_decay, default_q, n_actions, q_tabel=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.start_eps_decay = start_eps_decay\n",
    "        self.end_eps_decay = end_eps_decay\n",
    "        self.eps_decay = eps_decay\n",
    "        \n",
    "        self.episode = 0\n",
    "        \n",
    "        if q_tabel is None:\n",
    "            self.q_tabel = defaultdict(lambda: [default_q for _ in range(self.n_actions)])\n",
    "        else:\n",
    "            self.q_tabel = q_tabel\n",
    "        \n",
    "    def update(self, current_state_key, future_state_key, reward, action, terminate_state):\n",
    "        old_value = self.q_tabel[current_state_key][action]\n",
    "        if not terminate_state:\n",
    "            estimate_of_optimal_furure_value = np.max(self.q_tabel[future_state_key])\n",
    "\n",
    "            learnd_value = reward + self.discount_factor * estimate_of_optimal_furure_value\n",
    "\n",
    "            new_q = (1 - self.learning_rate) * old_value + self.learning_rate * learnd_value\n",
    "            \n",
    "        else:\n",
    "            new_q = reward\n",
    "            self.__decay_epsilon()\n",
    "            self.episode += 1\n",
    "        \n",
    "        #self.new_q_hist.append(new_q)\n",
    "        new_action_qs = self.q_tabel[current_state_key]\n",
    "        new_action_qs[action] = new_q\n",
    "        \n",
    "        self.q_tabel[current_state_key] = new_action_qs\n",
    "        \n",
    "        \n",
    "    def act(self, state_key):\n",
    "        q_values = self.q_tabel[state_key]\n",
    "        \n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def __decay_epsilon(self):\n",
    "        if self.start_eps_decay <= self.episode <=  self.end_eps_decay:\n",
    "            self.epsilon -= self.eps_decay\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end_eps_decay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2587947b95a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mend_eps_decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'end_eps_decay' is not defined"
     ]
    }
   ],
   "source": [
    "end_eps_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     0, average reward: -0.2, current epsilon: 1.00\n",
      "Episode:  1000, average reward: -200.0, current epsilon: 0.92\n",
      "Episode:  2000, average reward: -200.0, current epsilon: 0.84\n",
      "Episode:  3000, average reward: -200.0, current epsilon: 0.76\n",
      "Episode:  4000, average reward: -200.0, current epsilon: 0.68\n",
      "Episode:  5000, average reward: -200.0, current epsilon: 0.60\n",
      "Episode:  6000, average reward: -200.0, current epsilon: 0.52\n",
      "Episode:  7000, average reward: -200.0, current epsilon: 0.44\n",
      "Episode:  8000, average reward: -199.9, current epsilon: 0.36\n",
      "Episode:  9000, average reward: -198.5, current epsilon: 0.28\n",
      "Episode: 10000, average reward: -196.1, current epsilon: 0.20\n",
      "Episode: 11000, average reward: -197.4, current epsilon: 0.12\n",
      "Episode: 12000, average reward: -197.7, current epsilon: 0.04\n",
      "Episode: 13000, average reward: -200.0, current epsilon: -0.00\n",
      "Episode: 14000, average reward: -199.7, current epsilon: -0.00\n",
      "Episode: 15000, average reward: -200.0, current epsilon: -0.00\n",
      "Episode: 16000, average reward: -200.0, current epsilon: -0.00\n",
      "Episode: 17000, average reward: -200.0, current epsilon: -0.00\n",
      "Episode: 18000, average reward: -200.0, current epsilon: -0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9f9832ed5e90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mfurure_state_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfurure_state_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9f9832ed5e90>\u001b[0m in \u001b[0;36mgen_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mnorm_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_max_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_lows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_high\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mdis_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_state\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstate_dis_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mstate_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdis_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9f9832ed5e90>\u001b[0m in \u001b[0;36mmin_max_normalization\u001b[1;34m(x, min_, max_)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmin_max_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "dis = 0.95\n",
    "epochs = 25000\n",
    "show_period = 3000\n",
    "start_period = 1000\n",
    "epsilon = 1\n",
    "start_eps_decay = 1\n",
    "end_eps_decay = epochs//2\n",
    "eps_decay = epsilon/(end_eps_decay-start_eps_decay)\n",
    "default_q = 0\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "state_dis_number = np.array([30, 30])\n",
    "action_numbers = env.action_space.n\n",
    "obs_lows = env.observation_space.low\n",
    "obs_high = env.observation_space.high\n",
    "\n",
    "def min_max_normalization(x, min_, max_):\n",
    "    return (x - min_) / (max_ - min_)\n",
    "\n",
    "def gen_state(state):\n",
    "    norm_state = min_max_normalization(state, obs_lows, obs_high)\n",
    "    dis_state = np.round(norm_state * state_dis_number).astype(np.int)\n",
    "    state_key = \",\".join(dis_state.astype(str))\n",
    "    \n",
    "    return state_key\n",
    "\n",
    "\n",
    "agent = QTabel(learning_rate=lr, discount_factor=dis, epsilon=epsilon, \n",
    "               start_eps_decay=start_eps_decay, end_eps_decay=end_eps_decay, eps_decay=eps_decay, \n",
    "               default_q=default_q, n_actions=action_numbers)\n",
    "\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': [], \"eps\":[]}\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    observation = env.reset()\n",
    "    state_key = gen_state(observation)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    epoch_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state_key)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        furure_state_key = gen_state(observation)\n",
    "        \n",
    "        agent.update(state_key, furure_state_key, reward, action, done)\n",
    "        \n",
    "        state_key = furure_state_key\n",
    "        \n",
    "        epoch_reward+=reward\n",
    "        \n",
    "        if not epoch%show_period:\n",
    "            env.render()\n",
    "        \n",
    "    ep_rewards.append(epoch_reward)\n",
    "    if not epoch % start_period:\n",
    "        average_reward = sum(ep_rewards[-start_period:])/start_period\n",
    "        aggr_ep_rewards['ep'].append(epoch)\n",
    "        aggr_ep_rewards['eps'].append(agent.epsilon)\n",
    "        aggr_ep_rewards['avg'].append(average_reward)\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-start_period:]))\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-start_period:]))\n",
    "        print(f'Episode: {epoch:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {agent.epsilon:>1.2f}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tabel_save = agent.q_tabel.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['eps'], label=\"epsilon\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.40273339,  -1.45018966,  -1.74370722],\n",
       "       [ -1.33127795,  -1.29668713,  -1.7805012 ],\n",
       "       [ -1.46816121,  -1.59922594,  -0.27674495],\n",
       "       [ -7.79523959,  -7.48251917,  -8.41090096],\n",
       "       [-13.60187352, -13.63800711, -13.60970914],\n",
       "       [-14.08275193, -14.16152965, -14.07109794],\n",
       "       [-14.37601709, -14.59615471, -14.54761468],\n",
       "       [-14.46209958, -14.64565814, -14.66465004],\n",
       "       [-14.57370265, -14.3992035 , -14.35420018],\n",
       "       [-14.28945065, -14.13121454, -13.75514367],\n",
       "       [-13.38135052, -13.32437914, -13.21673111],\n",
       "       [-12.54831647, -12.49321792, -12.52558455],\n",
       "       [-11.89256313, -12.03761798, -11.8861876 ],\n",
       "       [-10.00561922,  -9.61682422,  -9.69301064],\n",
       "       [ -0.37022456,  -1.53670722,  -1.84607239],\n",
       "       [ -1.16404863,  -0.92855513,  -0.44366157],\n",
       "       [ -0.46749058,  -1.56446286,  -1.10028517],\n",
       "       [ -0.56603822,  -0.56605193,  -1.63759311],\n",
       "       [ -1.79325344,  -0.93794518,  -1.67646484],\n",
       "       [ -0.29913288,  -0.52046081,  -1.13045447]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode:     0, average reward: -0.2, current epsilon: 1.00\n",
      "Episode:  1000, average reward: -200.0, current epsilon: 0.92\n",
      "Episode:  2000, average reward: -200.0, current epsilon: 0.84\n",
      "3000\n",
      "Episode:  3000, average reward: -200.0, current epsilon: 0.76\n",
      "Episode:  4000, average reward: -200.0, current epsilon: 0.68\n",
      "Episode:  5000, average reward: -200.0, current epsilon: 0.60\n",
      "6000\n",
      "Episode:  6000, average reward: -200.0, current epsilon: 0.52\n",
      "Episode:  7000, average reward: -199.9, current epsilon: 0.44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fd6e27036ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mnew_discrete_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_discrete_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mSHOW_EVERY\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-fd6e27036ff2>\u001b[0m in \u001b[0;36mget_discrete_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_discrete_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mdiscrete_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdiscrete_os_win_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscrete_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# we use this tuple to look up the 3 Q values for the available actions in the q-table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# objective is to get the cart to the flag.\n",
    "# for now, let's just move randomly:\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 25000\n",
    "SHOW_EVERY = 3000\n",
    "STATS_EVERY = 1000\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, qoing to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n",
    "\n",
    "# For stats\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []}\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            \n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            #q_table[discrete_state + (action,)] = reward\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % STATS_EVERY:\n",
    "        average_reward = sum(ep_rewards[-STATS_EVERY:])/STATS_EVERY\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(average_reward)\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-STATS_EVERY:]))\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-STATS_EVERY:]))\n",
    "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
